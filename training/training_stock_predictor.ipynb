{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19228a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fb5ed",
   "metadata": {},
   "source": [
    "First, we’ll identify a target that we’re trying to predict. Our target will be if the next close price will go up or down tomorrow. If the price went up, the target will be 1.0, and if it went down, the target will be 0.0.\n",
    "\n",
    "EMA explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533da95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "def prepare_stock_data(stock, start_date='2018-01-01', end_date='2021-01-18'):\n",
    "    df = pd.read_csv('./data/' + stock + '_stock_data_with_polarity.csv')\n",
    "\n",
    "    # Set date as index\n",
    "    df['Date'] = df['Date'].astype(str).str.split(' ').str[0]\n",
    "    df = df.set_index('Date')\n",
    "    \n",
    "    # Add label\n",
    "    df['Label'] = df.rolling(2).apply(lambda x: x.iloc[1] > x.iloc[0])['Close']\n",
    "    \n",
    "    # Shift one day, we can not use the future to predict the past\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n",
    "    \n",
    "    df = df.rename(columns={'Open': 'Prev Open', 'High': 'Prev High', 'Low': 'Prev Low', \n",
    "                            'Close': 'Prev Close', 'Volume': 'Prev Volume', })\n",
    "    \n",
    "    # Add sentiment analysis\n",
    "\n",
    "    # Compute Exponential Mobile Average (EMA) for stock price daily increments\n",
    "    delta = df['Prev Close'] - df['Prev Open']\n",
    "    df['10 Days Incr EMA'] = np.round(delta.copy().ewm(span=10, adjust=False).mean(), decimals=3)\n",
    "    df['5 Days Incr EMA'] = np.round(delta.copy().ewm(span=5, adjust=False).mean(), decimals=3)\n",
    "    df['3 Days Incr EMA'] = np.round(delta.copy().ewm(span=3, adjust=False).mean(), decimals=3)\n",
    "    \n",
    "    # Compute Exponential Mobile Average (EMA) for stock polarity\n",
    "    df['10 Days Pol EMA'] = np.round(df['Polarity'].copy().ewm(span=10, adjust=False).mean(), decimals=3)\n",
    "    df['5 Days Pol EMA'] = np.round(df['Polarity'].copy().ewm(span=5, adjust=False).mean(), decimals=3)\n",
    "    df['3 Days Pol EMA'] = np.round(df['Polarity'].copy().ewm(span=3, adjust=False).mean(), decimals=3)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Re order columns\n",
    "    #df = df[['Date', 'Stock Trend EMA', 'S&P 500 Trend EMA', 'Label']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de790e62",
   "metadata": {},
   "source": [
    "the training and test set have to follow chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c39e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set:  (732, 9)\n",
      "Size of test set:  (316, 9)\n",
      "Size of train set:  (732, 1)\n",
      "Size of test set:  (316, 1)\n"
     ]
    }
   ],
   "source": [
    "predictors = [#'Prev Open', #NO\n",
    "              #'Prev High', #NO\n",
    "              #'Prev Low', #NO\n",
    "              'Prev Close', # SI\n",
    "              'Prev Volume', # SI\n",
    "              'Polarity',\n",
    "              '10 Days Incr EMA', # SI\n",
    "              '5 Days Incr EMA', # SI\n",
    "              '3 Days Incr EMA', # SI\n",
    "              '10 Days Pol EMA', # SI\n",
    "              '5 Days Pol EMA', # SI\n",
    "              '3 Days Pol EMA' # SI\n",
    "            ]\n",
    "\n",
    "training_stocks = ['AMZN', 'AAPL', 'MSFT', 'GOOGL']  # GOOGL\n",
    "\n",
    "x_train = pd.DataFrame()\n",
    "x_test = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "\n",
    "for train_stock in training_stocks:\n",
    "    \n",
    "    df = prepare_stock_data(train_stock)\n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(df[predictors],\n",
    "                                                df[['Label']], test_size=.3,\n",
    "                                                shuffle=False, random_state=0)\n",
    "    \n",
    "    x_train = x_train.append(x_tr, ignore_index=True)\n",
    "    x_test = x_test.append(x_te, ignore_index=True)\n",
    "    y_train = y_train.append(y_tr, ignore_index=True)\n",
    "    y_test = y_test.append(y_te, ignore_index=True)\n",
    "\n",
    "print('Size of train set: ', x_train.shape)\n",
    "print('Size of test set: ', x_test.shape)\n",
    "print('Size of train set: ', y_train.shape)\n",
    "print('Size of test set: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1a40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Spot-Check Algorithms\n",
    "classifiers = [\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    (\"pca\", PCA())\n",
    "])\n",
    "\n",
    "numeric_features = predictors\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0824b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  RandomForestClassifier()\n",
      "Accuracy on test set:  0.5537974683544303\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.38      0.45       154\n",
      "         1.0       0.55      0.72      0.62       162\n",
      "\n",
      "    accuracy                           0.55       316\n",
      "   macro avg       0.56      0.55      0.54       316\n",
      "weighted avg       0.56      0.55      0.54       316\n",
      "\n",
      "[19:56:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1637426408905/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "Training  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "Accuracy on test set:  0.5316455696202531\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.42      0.47       154\n",
      "         1.0       0.54      0.64      0.58       162\n",
      "\n",
      "    accuracy                           0.53       316\n",
      "   macro avg       0.53      0.53      0.52       316\n",
      "weighted avg       0.53      0.53      0.53       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.conda/envs/pythonProject/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  AdaBoostClassifier()\n",
      "Accuracy on test set:  0.5506329113924051\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.43      0.48       154\n",
      "         1.0       0.55      0.67      0.60       162\n",
      "\n",
      "    accuracy                           0.55       316\n",
      "   macro avg       0.55      0.55      0.54       316\n",
      "weighted avg       0.55      0.55      0.54       316\n",
      "\n",
      "\n",
      "Training  KNeighborsClassifier()\n",
      "Accuracy on test set:  0.5063291139240507\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.43      0.46       154\n",
      "         1.0       0.52      0.58      0.55       162\n",
      "\n",
      "    accuracy                           0.51       316\n",
      "   macro avg       0.50      0.50      0.50       316\n",
      "weighted avg       0.50      0.51      0.50       316\n",
      "\n",
      "\n",
      "Training  DecisionTreeClassifier()\n",
      "Accuracy on test set:  0.5253164556962026\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.45      0.48       154\n",
      "         1.0       0.53      0.59      0.56       162\n",
      "\n",
      "    accuracy                           0.53       316\n",
      "   macro avg       0.52      0.52      0.52       316\n",
      "weighted avg       0.52      0.53      0.52       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:  \n",
    "    \n",
    "    pipe = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('classifier', classifier)\n",
    "           ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipe.fit(x_train, y_train.values.ravel())\n",
    "    \n",
    "    # Use model to make predictions\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    \n",
    "    # Evaluate the performance\n",
    "    print(\"\\nTraining \", classifier)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "    print(\"Metrics per class on test set:\")\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a106ce6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['../model/classification_model.pkl']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the Model to disk\n",
    "filename = '../model/classification_model.pkl'\n",
    "final_pipe = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('classifier', classifiers[0])\n",
    "           ])\n",
    "joblib.dump(final_pipe, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead0de7",
   "metadata": {},
   "source": [
    "Evaluate the model using other test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf033a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stock_value(stock):\n",
    "    df = prepare_stock_data(stock)\n",
    "    y_test = df['Label']\n",
    "    \n",
    "    # Load the Regression Model\n",
    "    pipe = joblib.load('../model/classification_model.pkl')\n",
    "    \n",
    "    # Use model to make predictions\n",
    "    y_pred = pipe.predict(df[predictors])\n",
    "\n",
    "    # Evaluate the performance\n",
    "    print(\"\\n Evaluating \", pipe['classifier'])\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print(\"Accuracy on test set: \", accuracy)\n",
    "    print(\"Metrics per class on test set:\")\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fb0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating  RandomForestClassifier()\n",
      "Accuracy on test set:  0.8587786259541985\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.73      0.82       113\n",
      "         1.0       0.82      0.96      0.89       149\n",
      "\n",
      "    accuracy                           0.86       262\n",
      "   macro avg       0.88      0.84      0.85       262\n",
      "weighted avg       0.87      0.86      0.86       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_stock_value('GOOGL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e420f8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd27be71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}