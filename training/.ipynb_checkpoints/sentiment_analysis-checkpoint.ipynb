{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beaed3b0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "In this notebook it is showed the workflow of how we built the sentiment analysis model to classify the polarity of the financial tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ade0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import joblib\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a0006",
   "metadata": {},
   "source": [
    "We used two different datasets of financial tweets to train and test our model:\n",
    "the first dataset was downloaded from Kaggle at the following link and it contains labelled tweets ;\n",
    "the test set includes real tweets that were scraped form Twitter and that we manually classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec786d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video offic mind busi david solomon tell gs in...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>price lumber lb f sinc hit ytd high maci turna...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say american dream dead</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barri silbert extrem optimist bitcoin predict ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>satellit avoid attack space junk circl earth paid</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28435</th>\n",
       "      <td>fb c f f cb ecf</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28436</th>\n",
       "      <td>btc</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28437</th>\n",
       "      <td>rt hd nuff said tel telcoin telfam crypto bloc...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28438</th>\n",
       "      <td>btc</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28439</th>\n",
       "      <td>stellar xlm price binanc registr open limit time</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28440 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    target\n",
       "0      video offic mind busi david solomon tell gs in...   neutral\n",
       "1      price lumber lb f sinc hit ytd high maci turna...   neutral\n",
       "2                                say american dream dead  negative\n",
       "3      barri silbert extrem optimist bitcoin predict ...  positive\n",
       "4      satellit avoid attack space junk circl earth paid  negative\n",
       "...                                                  ...       ...\n",
       "28435                                    fb c f f cb ecf   neutral\n",
       "28436                                                btc   neutral\n",
       "28437  rt hd nuff said tel telcoin telfam crypto bloc...   neutral\n",
       "28438                                                btc   neutral\n",
       "28439   stellar xlm price binanc registr open limit time   neutral\n",
       "\n",
       "[28440 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing.tweet_cleaner import tweet_pruning, remove_special_char\n",
    "\n",
    "train_data = pd.read_csv('./data/tweets_with_sentiment.csv')\n",
    "\n",
    "# Preprocessing\n",
    "train_data['text'] = train_data['text'].astype(str)\n",
    "train_data['text'] = train_data['text'].str.lower()\n",
    "train_data['text'] = train_data['text'].apply(remove_special_char)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d28b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Spot-Check Algorithms\n",
    "classifiers = [\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(eval_metric='mlogloss'),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB()\n",
    "]\n",
    "\n",
    "# Pipeline Classifier\n",
    "pipelines = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    \n",
    "    pipelines.append(Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', classifier)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a74a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before undersampling:  Counter({'neutral': 13883, 'positive': 6785, 'negative': 2084})\n",
      "After undersampling:  Counter({'neutral': 13883, 'positive': 6785, 'negative': 2084})\n"
     ]
    }
   ],
   "source": [
    "print(\"Before undersampling: \", Counter(y_train))\n",
    "\n",
    "# Convert x_train to np_array for rebalance\n",
    "#x_train = x_train.values.reshape(-1, 1)\n",
    "#x_train, y_train = make_imbalance(x_train, y_train,\n",
    "                                  #sampling_strategy={'positive': 2000, 'neutral': 2000, 'negative': 2000},\n",
    "                                  #random_state=0)\n",
    "\n",
    "# Return to pandas series\n",
    "x_train = pd.Series(np.squeeze(x_train))\n",
    "print(\"After undersampling: \", Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adf60d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation:  RandomForestClassifier()  \tTraining time:  24.588970875740053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.83      0.89      2598\n",
      "     neutral       0.96      0.98      0.97     17330\n",
      "    positive       0.96      0.95      0.96      8512\n",
      "\n",
      "    accuracy                           0.96     28440\n",
      "   macro avg       0.96      0.92      0.94     28440\n",
      "weighted avg       0.96      0.96      0.96     28440\n",
      "\n",
      "\n",
      " Evaluation:  XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss', gamma=None,\n",
      "              gpu_id=None, importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)  \tTraining time:  9.829106831550598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.77      0.84      2598\n",
      "     neutral       0.93      0.98      0.96     17330\n",
      "    positive       0.97      0.92      0.94      8512\n",
      "\n",
      "    accuracy                           0.94     28440\n",
      "   macro avg       0.94      0.89      0.91     28440\n",
      "weighted avg       0.95      0.94      0.94     28440\n",
      "\n",
      "\n",
      " Evaluation:  AdaBoostClassifier()  \tTraining time:  4.43144919872284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.61      0.73      2598\n",
      "     neutral       0.86      0.98      0.92     17330\n",
      "    positive       0.95      0.79      0.86      8512\n",
      "\n",
      "    accuracy                           0.89     28440\n",
      "   macro avg       0.91      0.79      0.84     28440\n",
      "weighted avg       0.89      0.89      0.88     28440\n",
      "\n",
      "\n",
      " Evaluation:  KNeighborsClassifier(n_neighbors=3)  \tTraining time:  2.4564626216888428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.34      0.45      2598\n",
      "     neutral       0.72      0.94      0.81     17330\n",
      "    positive       0.83      0.43      0.57      8512\n",
      "\n",
      "    accuracy                           0.73     28440\n",
      "   macro avg       0.74      0.57      0.61     28440\n",
      "weighted avg       0.75      0.73      0.71     28440\n",
      "\n",
      "\n",
      " Evaluation:  LogisticRegression()  \tTraining time:  3.334597134590149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.64      0.76      2598\n",
      "     neutral       0.90      0.98      0.94     17330\n",
      "    positive       0.95      0.88      0.91      8512\n",
      "\n",
      "    accuracy                           0.92     28440\n",
      "   macro avg       0.93      0.83      0.87     28440\n",
      "weighted avg       0.92      0.92      0.91     28440\n",
      "\n",
      "\n",
      " Evaluation:  MultinomialNB()  \tTraining time:  0.5739683628082275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.20      0.33      2598\n",
      "     neutral       0.78      0.96      0.86     17330\n",
      "    positive       0.85      0.66      0.74      8512\n",
      "\n",
      "    accuracy                           0.80     28440\n",
      "   macro avg       0.86      0.60      0.64     28440\n",
      "weighted avg       0.82      0.80      0.77     28440\n",
      "\n",
      "\n",
      " Evaluation:  BernoulliNB()  \tTraining time:  0.5728644847869873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.40      0.52      2598\n",
      "     neutral       0.86      0.89      0.88     17330\n",
      "    positive       0.80      0.84      0.82      8512\n",
      "\n",
      "    accuracy                           0.83     28440\n",
      "   macro avg       0.79      0.71      0.74     28440\n",
      "weighted avg       0.83      0.83      0.83     28440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for pipe in pipelines:\n",
    "    t0 = time.time()\n",
    "    predicted = cross_val_predict(pipe, train_data['text'], train_data['target'], cv=10)\n",
    "    t1 = time.time()\n",
    "    t = (t1-t0)/10\n",
    "    \n",
    "    print(\"\\n Evaluation: \", pipe['clf'], \" \\tTraining time: \", t)\n",
    "    print(metrics.classification_report(train_data['target'], predicted, target_names=[\"negative\", \"neutral\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a1e2a",
   "metadata": {},
   "source": [
    "The performance using Kaggle dataset were excellent but we want to test our model also with tweets that we scraped directly from Twitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d75c6d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets:  Counter({'neutral': 566, 'positive': 143, 'negative': 50})\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15368/3594654729.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1374\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             )\n\u001b[1;32m-> 1376\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "# Testing with real tweets\n",
    "real_tweets = pd.read_json('../data/train/tweets_with_label.json')\n",
    "print(\"Number of Tweets: \", Counter(real_tweets['target']))\n",
    "\n",
    "# Preprocessing\n",
    "real_tweets = real_tweets.rename(columns={'text': 'Text'})\n",
    "real_tweets['Text'] = real_tweets['Text'].str.lower()\n",
    "real_tweets = tweet_pruning(real_tweets, 'amazon', 'AMZN')\n",
    "real_tweets['Text'] = real_tweets['Text'].apply(remove_special_char)\n",
    "\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(train_data['text'], train_data['target'])\n",
    "    \n",
    "    t0 = time.time()    \n",
    "    predicted = pipe.predict(real_tweets['Text'].values)\n",
    "    t1 = time.time()\n",
    "    t = (t1-t0)\n",
    "    \n",
    "    print(\"\\n Evaluation: \", pipe['clf'], \" \\tPrediction time: \", t)\n",
    "    print(metrics.classification_report(real_tweets['target'].values, predicted, target_names=[\"negative\", \"neutral\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007fbd9",
   "metadata": {},
   "source": [
    "XGBoost Classifier has the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21248433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/sentiment_classifier.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the classifier\n",
    "filename = '../model/sentiment_classifier.pkl'\n",
    "joblib.dump(pipelines[1], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e50cae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets:  Counter({'neutral': 566, 'positive': 143, 'negative': 50})\n",
      "Accuracy on test set:  0.7926829268292683\n",
      "Metrics per class on test set:\n",
      "Confusion matrix:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.63      0.71        43\n",
      "     neutral       0.83      0.90      0.86        70\n",
      "    positive       0.73      0.78      0.75        51\n",
      "\n",
      "    accuracy                           0.79       164\n",
      "   macro avg       0.79      0.77      0.78       164\n",
      "weighted avg       0.79      0.79      0.79       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = joblib.load('../model/sentiment_classifier.pkl')\n",
    "\n",
    "# Testing with real tweets\n",
    "real_tweets = pd.read_json('../data/train/tweets_with_label.json')\n",
    "print(\"Number of Tweets: \", Counter(real_tweets['target']))\n",
    "\n",
    "# Preprocessing\n",
    "real_tweets = real_tweets.rename(columns={'text': 'Text'})\n",
    "real_tweets['Text'] = real_tweets['Text'].str.lower()\n",
    "real_tweets = tweet_pruning(real_tweets, 'amazon', 'AMZN')\n",
    "real_tweets['Text'] = real_tweets['Text'].apply(remove_special_char)\n",
    "\n",
    "# Predicting\n",
    "predicted = pipe.predict(real_tweets['Text'].values)\n",
    "\n",
    "\n",
    "# Extracting statistics and metrics\n",
    "accuracy = accuracy_score(real_tweets['target'], predicted)\n",
    "print(\"Accuracy on test set: \", accuracy)\n",
    "print(\"Metrics per class on test set:\")\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "metrics.confusion_matrix(real_tweets['target'].values, predicted)\n",
    "\n",
    "print(metrics.classification_report(real_tweets['target'].values, predicted, target_names=[\"negative\", \"neutral\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d659e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56b197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
